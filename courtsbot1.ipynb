{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1pfCGMqkn7Kg4G4HMWmHhtDa9ucWgonjj",
      "authorship_tag": "ABX9TyPhrbqjvRT0b4LjwAlBdvsH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dongjaeseo/NLP_study/blob/main/courtsbot1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsummary"
      ],
      "metadata": {
        "id": "jvSCTdF2PF57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torchsummary import summary\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "CTYscQ3_OGZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(transformers.__version__)"
      ],
      "metadata": {
        "id": "urccCWXJrxqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, texts, labels, max_length=32):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.max_length = max_length\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = int(self.labels[idx])\n",
        "\n",
        "        # Tokenize the text\n",
        "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
        "\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        attention_mask = encoding['attention_mask'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Your dataset\n",
        "# texts = ['i am happy', 'i am sad', 'movie is good', 'hes a bad guy', 'what a good day', 'gloomy day', 'I hate this movie', 'i just love this coffee', 'i dont want to go to class today', 'creepy idea', 'lovely cat', 'great idea']\n",
        "# labels = [1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1]\n",
        "texts = ['what slots are available for this court on 26th?',\n",
        "         'can I know which timings are available on Friday?',\n",
        "         'Which are the possible slots for next Wednesday?',\n",
        "         'when can i book court on 17th?',\n",
        "         'how can i contact with you?',\n",
        "         'i would like to call someone incharge',\n",
        "         'I dont understand the process. Could I contact you?',\n",
        "         'Can you send me an email?',\n",
        "         'how can i make payment?',\n",
        "         'can I pay through bank account transfer?',\n",
        "         'I want to make monthly subscription',\n",
        "         'Which payment options do you offer?']\n",
        "\n",
        "labels = [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2]\n",
        "\n",
        "\n",
        "# Create an instance of your CustomDataset\n",
        "custom_dataset = CustomDataset(texts, labels)\n",
        "\n",
        "# Create a DataLoader\n",
        "batch_size = 2  # You can adjust this based on your needs\n",
        "custom_dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Example usage\n",
        "for batch in custom_dataloader:\n",
        "    print(batch)"
      ],
      "metadata": {
        "id": "cLcuBQdubj06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Sample dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, texts, labels, max_length=32):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.max_length = max_length\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = int(self.labels[idx])\n",
        "\n",
        "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
        "\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        attention_mask = encoding['attention_mask'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Your dataset\n",
        "texts = ['What slots are available for booking this Friday?',\n",
        "         'can I know which timings are available on Friday?',\n",
        "         'Which are the possible slots for next Wednesday?',\n",
        "         'when can i book court on 17th?',\n",
        "         'how can i contact with you?',\n",
        "         'i would like to call someone in charge',\n",
        "         'I dont understand the process. Could I contact you?',\n",
        "         'Can you send me an email?',\n",
        "         'how can i make payment?',\n",
        "         'can I pay through bank account transfer?',\n",
        "         'I want to make a monthly subscription',\n",
        "         'Which payment options do you offer?']\n",
        "\n",
        "labels = [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2]\n",
        "\n",
        "custom_dataset = CustomDataset(texts, labels)\n",
        "batch_size = 2\n",
        "custom_dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Load pre-trained BERT model for sequence classification\n",
        "model_name = 'bert-base-uncased'\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)  # Assuming 3 classes\n",
        "\n",
        "# Set up optimizer and loss function\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20  # Adjust as needed\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(custom_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        batch_labels = batch['labels'].to(torch.long).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_ids, attention_mask=attention_mask).logits\n",
        "        loss = criterion(logits, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    average_loss = total_loss / len(custom_dataloader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {average_loss:.4f}\")\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained('fine_tuned_bert_model')"
      ],
      "metadata": {
        "id": "FY-HPFfusFoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the fine-tuned BERT model\n",
        "fine_tuned_model = BertForSequenceClassification.from_pretrained('fine_tuned_bert_model', num_labels=3)  # Assuming 3 classes\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "fine_tuned_model.eval()\n",
        "\n",
        "# Example texts for inference\n",
        "example_texts = [\n",
        "    'what slots are available for this court on 26th?',\n",
        "    'can I know which timings are available on Friday?',\n",
        "    'what are possible timings on next Sunday?',\n",
        "    'Can you send me an email?',\n",
        "    'I would like to talk to you',\n",
        "    'Can we have a talk',\n",
        "    'Please drop me an email',\n",
        "    'Please drop me a message',\n",
        "    'i want to message you',\n",
        "    'how to make transaction',\n",
        "    'payment portal isnt there',\n",
        "    'I want to make payment',\n",
        "\n",
        "    'when can i pay',\n",
        "    'i want to talk regarding payment',\n",
        "    'i want to talk regarding when to book'\n",
        "]\n",
        "\n",
        "# Tokenize and process the example texts\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "inputs = tokenizer(example_texts, return_tensors='pt', truncation=True, padding=True, max_length=32)\n",
        "\n",
        "# Move inputs to the appropriate device (CPU or GPU)\n",
        "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "# Forward pass to get logits\n",
        "with torch.no_grad():\n",
        "    logits = fine_tuned_model(**inputs).logits\n",
        "\n",
        "# Apply softmax to get probabilities\n",
        "probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "\n",
        "# Get predicted labels\n",
        "predicted_labels = torch.argmax(probs, dim=1).cpu().numpy()\n",
        "\n",
        "# Print the results\n",
        "for i, text in enumerate(example_texts):\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Predicted Label: {predicted_labels[i]}\")\n",
        "    print(f\"Probabilities: {probs[i]}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "mJrsaxLyceck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rf2M0R-0Zebv"
      }
    }
  ]
}